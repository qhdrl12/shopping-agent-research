"""
Enhanced Shopping Agent with Pre-Search and Pre-Scraping Pipeline

ì´ ëª¨ë“ˆì€ LangGraphë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœ ê³ ê¸‰ ì‡¼í•‘ ì¶”ì²œ ì—ì´ì „íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ê¸°ì¡´ React Agentì™€ ë‹¬ë¦¬ êµ¬ì¡°í™”ëœ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ë” ì •í™•í•˜ê³  ìƒì„¸í•œ ì‡¼í•‘ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” íŠ¹ì§•:
- ğŸ“Š êµ¬ì¡°í™”ëœ ì§ˆë¬¸ ë¶„ì„ (Structured Output í™œìš©)
- ğŸ” ì„¤ì • ê¸°ë°˜ ì‚¬ì „ ê²€ìƒ‰ (Tavily API)
- ğŸ•·ï¸ ìŠ¤ë§ˆíŠ¸í•œ URL ì„ ë³„ ë° ìŠ¤í¬ë˜í•‘ (Firecrawl API)
- ğŸ¯ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìµœì¢… ì¶”ì²œ ìƒì„±
- ğŸ¨ UI ì¹œí™”ì  ë„êµ¬ ì¶”ì  (LangGraph astream_events í™œìš©)

ì›Œí¬í”Œë¡œìš°:
1. analyze_query: ì‚¬ìš©ì ì§ˆë¬¸ì„ êµ¬ì¡°í™”ëœ ì •ë³´ë¡œ ë¶„ì„
2. pre_search: Tavilyë¥¼ í†µí•œ ê´€ë ¨ ì •ë³´ ìˆ˜ì§‘
3. pre_scrape: Firecrawlì„ í†µí•œ ìƒì„¸ ì½˜í…ì¸  ìˆ˜ì§‘
4. react_agent: ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì  ì¶”ì²œ ì œê³µ

ê°œì„ ëœ UI ì¶”ì :
- ê° ë„êµ¬ í˜¸ì¶œì´ on_tool_start/end ì´ë²¤íŠ¸ë¥¼ ë°œìƒì‹œì¼œ ì‹¤ì‹œê°„ UI ì¶”ì  ê°€ëŠ¥
- React Agentì™€ ë™ì¼í•œ ë„êµ¬ ì´ë ¥ í‘œì‹œ ê²½í—˜ ì œê³µ
- Human â†’ ToolMessage â†’ AI ìˆœì„œì˜ ì¼ê´€ëœ ë©”ì‹œì§€ í”Œë¡œìš°
"""

import os
import asyncio
import json
from typing import Annotated, TypedDict, List, Dict, Any, Optional
from datetime import datetime

from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.graph.state import CompiledStateGraph
from dotenv import load_dotenv
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from config.agent_config import AgentConfig, get_config
from utils.local_prompt_manager import LocalPromptManager
from utils.text_processing import (
    extract_title_from_content,
    extract_product_info_from_content,
    calculate_relevance_score
)

load_dotenv()


class BasicAnalysis(BaseModel):
    """ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ê²°ê³¼"""
    main_product: str = Field(description="ì£¼ìš” ìƒí’ˆëª…")
    search_keywords: List[str] = Field(description="ê²€ìƒ‰ í‚¤ì›Œë“œ ìµœëŒ€ 5ê°œ, ì¤‘ìš”ë„ ìˆœ", max_items=5)
    price_range: str = Field(description="ê°€ê²©ëŒ€ ì •ë³´")
    target_categories: List[str] = Field(description="ëŒ€ìƒ ì¹´í…Œê³ ë¦¬")
    search_intent: str = Field(description="ê²€ìƒ‰ ì˜ë„ (êµ¬ë§¤, ë¹„êµ, ì •ë³´ìˆ˜ì§‘, ì¶”ì²œ)")

class RoutingStrategy(BaseModel):
    """ë¼ìš°íŒ… ì „ëµ ê²°ì • ê²°ê³¼"""
    complexity_level: str = Field(description="ë³µì¡ë„ ìˆ˜ì¤€ (ë‹¨ìˆœ, ì¤‘ê°„, ë³µì¡, ë§¤ìš°ë³µì¡)")
    information_depth: str = Field(description="ì •ë³´ ê¹Šì´ (ê¸°ë³¸, ìƒì„¸, ì „ë¬¸)")
    routing_decision: str = Field(description="ë¼ìš°íŒ… ê²°ì • (simple_search, detailed_search, comprehensive_search)")
    recommended_sources: List[str] = Field(description="ì¶”ì²œ ê²€ìƒ‰ ì†ŒìŠ¤")
    scraping_targets: List[str] = Field(description="ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ")

class ExecutionPlan(BaseModel):
    """ì‹¤í–‰ ê³„íš"""
    primary_search_query: str = Field(description="ì£¼ìš” ê²€ìƒ‰ ì¿¼ë¦¬")
    secondary_search_queries: List[str] = Field(description="ë³´ì¡° ê²€ìƒ‰ ì¿¼ë¦¬")
    expected_results_count: int = Field(description="ì˜ˆìƒ ê²°ê³¼ ìˆ˜")
    scraping_priority: List[str] = Field(description="ìŠ¤í¬ë˜í•‘ ìš°ì„ ìˆœìœ„")

class QueryAnalysis(BaseModel):
    """ê³ ë„í™”ëœ ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼"""
    basic_analysis: BasicAnalysis
    routing_strategy: RoutingStrategy
    execution_plan: ExecutionPlan


class ShoppingAgentState(TypedDict):
    """Enhanced Shopping Agent ìƒíƒœ ê´€ë¦¬"""
    messages: Annotated[list[BaseMessage], add_messages]
    user_query: str
    
    # ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼ (ê³ ë„í™”ëœ êµ¬ì¡°)
    analyzed_query: Dict[str, Any]
    routing_decision: str  # simple_search, detailed_search, comprehensive_search
    search_keywords: List[str]
    target_categories: List[str]
    
    # ì‚¬ì „ ê²€ìƒ‰ ê²°ê³¼
    search_results: List[Dict[str, Any]]
    relevant_urls: List[str]
    
    # ì‚¬ì „ ìŠ¤í¬ë˜í•‘ ê²°ê³¼
    scraped_content: Dict[str, Any]
    product_data: List[Dict[str, Any]]
    
    # React Agent ì»¨í…ìŠ¤íŠ¸
    enriched_context: str
    
    # ìµœì¢… ê²°ê³¼
    final_answer: str
    processing_status: str
    error_info: Optional[str]


class EnhancedShoppingAgent:
    """
    Enhanced Shopping Agent - UI ì¶”ì  ì§€ì› ê³ ê¸‰ ì‡¼í•‘ ì¶”ì²œ ì—ì´ì „íŠ¸
    
    ì´ í´ë˜ìŠ¤ëŠ” LangGraph StateGraphë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœ 4ë‹¨ê³„ ì‡¼í•‘ ì¶”ì²œ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.
    ê° ë‹¨ê³„ì—ì„œ LangChain ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ UIì—ì„œ ì‹¤ì‹œê°„ ì§„í–‰ìƒí™©ì„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ì›Œí¬í”Œë¡œìš° ë‹¨ê³„:
    1. ğŸ“ ì§ˆë¬¸ ë¶„ì„ (analyze_query): 
       - Structured Outputì„ í†µí•œ ì•ˆì •ì ì¸ ì¿¼ë¦¬ íŒŒì‹±
       - ê²€ìƒ‰ í‚¤ì›Œë“œ, ìƒí’ˆ ì¹´í…Œê³ ë¦¬, ê°€ê²©ëŒ€ ë“± ì¶”ì¶œ
       - ì‡¼í•‘ ì˜ë„ ë¶„ì„ (êµ¬ë§¤, ë¹„êµ, ì •ë³´ìˆ˜ì§‘ ë“±)
       
    2. ğŸ” ì‚¬ì „ ê²€ìƒ‰ (pre_search):
       - Tavily APIë¥¼ í†µí•œ ê´€ë ¨ ì •ë³´ ìˆ˜ì§‘
       - ì„¤ì • ê¸°ë°˜ ê²€ìƒ‰ ê°œìˆ˜ ì œí•œ (API ë¹„ìš© ìµœì í™”)
       - ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ë° ê²°ê³¼ ì •ë ¬
       
    3. ğŸ•·ï¸ ì‚¬ì „ ìŠ¤í¬ë˜í•‘ (pre_scrape):
       - Firecrawl APIë¥¼ í†µí•œ ìƒì„¸ ì½˜í…ì¸  ìˆ˜ì§‘
       - ìŠ¤ë§ˆíŠ¸í•œ URL ì„ ë³„ (ê´€ë ¨ì„± ì ìˆ˜ + ì‡¼í•‘ëª° ë„ë©”ì¸ ìš°ì„ ìˆœìœ„)
       - ì½˜í…ì¸  ê¸¸ì´ ì œí•œ ë° ìƒí’ˆ ì •ë³´ ìë™ ì¶”ì¶œ
       
    4. ğŸ¯ ìµœì¢… ë‹µë³€ (react_agent):
       - ìˆ˜ì§‘ëœ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
       - ì „ë¬¸ ì‡¼í•‘ ì»¨ì„¤í„´íŠ¸ í˜ë¥´ì†Œë‚˜ë¡œ ìƒì„¸ ì¶”ì²œ ìƒì„±
       - ê°€ê²©ëŒ€ë³„ ì˜µì…˜, êµ¬ë§¤ ê°€ì´ë“œ, ëŒ€ì•ˆ ìƒí’ˆ ì œì‹œ
    
    UI ì¶”ì  ê¸°ëŠ¥:
        - ê° ë„êµ¬ í˜¸ì¶œ ì‹œ on_tool_start/end ì´ë²¤íŠ¸ ìë™ ë°œìƒ
        - app.pyì˜ astream_eventsì™€ ì™„ë²½ í˜¸í™˜
        - Human â†’ ToolMessage â†’ AI ìˆœì„œì˜ ì¼ê´€ëœ ë©”ì‹œì§€ í”Œë¡œìš°
        - React Agentì™€ ë™ì¼í•œ ë„êµ¬ ì´ë ¥ í‘œì‹œ ê²½í—˜
    
    ì„¤ì • ê¸°ë°˜ ìµœì í™”:
        - AgentConfigë¥¼ í†µí•œ ì„¸ë°€í•œ ë™ì‘ ì œì–´
        - ê²€ìƒ‰/ìŠ¤í¬ë˜í•‘ ê°œìˆ˜ ì œí•œìœ¼ë¡œ ë¹„ìš© ìµœì í™”
        - ì½˜í…ì¸  ê¸¸ì´ ì œí•œìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
        - ì—ëŸ¬ ë°œìƒ ì‹œ ìš°ì•„í•œ ì‹¤íŒ¨ ì²˜ë¦¬
    
    Example:
        >>> config = get_config("credit_saving")
        >>> agent = EnhancedShoppingAgent(config)
        >>> workflow = agent.create_workflow()
        >>> result = await workflow.ainvoke({
        ...     "user_query": "ê²¨ìš¸ìš© íŒ¨ë”© ì¶”ì²œí•´ì¤˜",
        ...     "messages": [],
        ...     "processing_status": "ì‹œì‘"
        ... })
    """
    
    def __init__(self, config: AgentConfig = None, prompt_name: str = "default"):
        """
        ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config (AgentConfig, optional): ì—ì´ì „íŠ¸ ì„¤ì • ê°ì²´. 
                                          Noneì¸ ê²½ìš° ê¸°ë³¸ ì„¤ì • ì‚¬ìš©.
            prompt_name (str, optional): Supabaseì—ì„œ ê°€ì ¸ì˜¬ í”„ë¡¬í”„íŠ¸ ì´ë¦„. ê¸°ë³¸ê°’ì€ "default".
        
        Note:
            - OpenAI, Tavily, Firecrawl API í‚¤ê°€ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨
            - ì„¤ì •ì„ í†µí•´ ê²€ìƒ‰/ìŠ¤í¬ë˜í•‘ ë²”ìœ„ ì¡°ì ˆ ê°€ëŠ¥
        """
        # ì„¤ì • ì´ˆê¸°í™” - ê¸°ë³¸ê°’ ë˜ëŠ” ì „ë‹¬ë°›ì€ ì„¤ì • ì‚¬ìš©
        self.config = config or get_config("default")
        
        # ë¡œì»¬ íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        self.prompt_manager = LocalPromptManager()
        self._load_prompts(prompt_name)

        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (OpenAI GPT ëª¨ë¸)
        self.llm = ChatOpenAI(
            model=self.config.llm_model,
            temperature=self.config.llm_temperature,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # LangChain ë„êµ¬ import ë° ì„¤ì •
        self.tools = self._setup_tools()

    def _load_prompts(self, prompt_name: str):
        """ë¡œì»¬ íŒŒì¼ì—ì„œ ì§€ì •ëœ ì´ë¦„ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        print(f'\nğŸ”„ ë¡œì»¬ íŒŒì¼ì—ì„œ "{prompt_name}" í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...')
        prompt_data = self.prompt_manager.get_prompt(prompt_name)
        if not prompt_data:
            raise ValueError(f'ë¡œì»¬ íŒŒì¼ì—ì„œ "{prompt_name}" í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')

        self.analysis_prompt_template = prompt_data.get('query_analysis_prompt', '')
        self.response_prompt_template = prompt_data.get('model_response_prompt', '')
        print("âœ… í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ.")
        
    def _setup_tools(self):
        """
        ì™¸ë¶€ ë„êµ¬ íŒŒì¼ì—ì„œ LangChain ë„êµ¬ë“¤ì„ importí•˜ê³  ì„¤ì •í•©ë‹ˆë‹¤.
        
        ì´ ë©”ì„œë“œëŠ” Enhanced Shopping Agentì—ì„œ ì‚¬ìš©í•  ë„êµ¬ë“¤ì„ ë™ì ìœ¼ë¡œ ë¡œë“œí•˜ê³ 
        ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ êµ¬ì„±í•˜ì—¬ ë…¸ë“œì—ì„œ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
        
        ê° ë„êµ¬ëŠ” @tool ë°ì½”ë ˆì´í„°ë¡œ ë˜í•‘ë˜ì–´ LangGraphì˜ astream_eventsì—ì„œ
        on_tool_start/end ì´ë²¤íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë°œìƒì‹œì¼œ UI ì¶”ì ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        
        Returns:
            Dict[str, Tool]: ë„êµ¬ ì´ë¦„ì„ í‚¤ë¡œ í•˜ëŠ” ë„êµ¬ ë”•ì…”ë„ˆë¦¬
                - "tavily_search_tool": ì›¹ ê²€ìƒ‰ ë„êµ¬
                - "firecrawl_scrape_tool": ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬
                
        Note:
            - ìƒˆë¡œìš´ ë„êµ¬ ì¶”ê°€ ì‹œ ì´ ë©”ì„œë“œë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤
            - ë„êµ¬ë“¤ì€ ë…ë¦½ì ì¸ íŒŒì¼ë¡œ ê´€ë¦¬ë˜ì–´ ì¬ì‚¬ìš©ì„±ì´ ë†’ìŠµë‹ˆë‹¤
            - ëª¨ë“  ë„êµ¬ëŠ” ì¼ê´€ëœ ì‘ë‹µ í˜•ì‹ì„ ì œê³µí•©ë‹ˆë‹¤ (success, error í¬í•¨)
        """
        # ì™¸ë¶€ ë„êµ¬ ëª¨ë“ˆì—ì„œ ë„êµ¬ í•¨ìˆ˜ë“¤ import
        from src.tools.tavily import tavily_search_tool      # Tavily ì›¹ ê²€ìƒ‰ ë„êµ¬
        from src.tools.firecrawl import firecrawl_scrape_tool # Firecrawl ìŠ¤í¬ë˜í•‘ ë„êµ¬
        
        # ë„êµ¬ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
        tools = [tavily_search_tool, firecrawl_scrape_tool]
        
        # ë„êµ¬ ì´ë¦„ì„ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ (ë…¸ë“œì—ì„œ self.tools["ë„êµ¬ëª…"]ìœ¼ë¡œ ì ‘ê·¼)
        return {tool.name: tool for tool in tools}
        
        
    def create_workflow(self) -> CompiledStateGraph:
        """
        ë¼ìš°íŒ… ê¸°ë°˜ LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤.
        
        Returns:
            CompiledStateGraph: ì‹¤í–‰ ê°€ëŠ¥í•œ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„
            
        Workflow:
            analyze_query â†’ [routing_decision] â†’ simple_search/detailed_search/comprehensive_search â†’ react_agent â†’ END
            
        Note:
            ë¼ìš°íŒ… ê²°ì •ì— ë”°ë¼ ë‹¤ë¥¸ ê²½ë¡œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        workflow = StateGraph(ShoppingAgentState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyze_query", self.analyze_query)
        workflow.add_node("simple_search", self.simple_search)
        workflow.add_node("detailed_search", self.detailed_search) 
        workflow.add_node("comprehensive_search", self.comprehensive_search)
        workflow.add_node("react_agent", self.call_agent)
        
        # ì§„ì…ì  ì„¤ì •
        workflow.set_entry_point("analyze_query")
        
        # ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì„¤ì •
        workflow.add_conditional_edges(
            "analyze_query",
            self.route_decision,
            {
                "simple_search": "simple_search",
                "detailed_search": "detailed_search", 
                "comprehensive_search": "comprehensive_search"
            }
        )
        
        # ê° ê²€ìƒ‰ ë…¸ë“œì—ì„œ ìµœì¢… ì‘ë‹µìœ¼ë¡œ
        workflow.add_edge("simple_search", "react_agent")
        workflow.add_edge("detailed_search", "react_agent")
        workflow.add_edge("comprehensive_search", "react_agent")
        workflow.add_edge("react_agent", END)
        
        return workflow.compile()
    
    def route_decision(self, state: ShoppingAgentState) -> str:
        """
        ë¶„ì„ ê²°ê³¼ì— ë”°ë¼ ë¼ìš°íŒ… ê²°ì •ì„ ë‚´ë¦½ë‹ˆë‹¤.
        
        Args:
            state: í˜„ì¬ ìƒíƒœ
            
        Returns:
            str: ë¼ìš°íŒ… ê²°ì • ("simple_search", "detailed_search", "comprehensive_search")
        """
        routing_decision = state.get("routing_decision", "detailed_search")
        print(f"ğŸ¯ ë¼ìš°íŒ… ê²°ì •: {routing_decision}")
        return routing_decision
    
    async def analyze_query(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """
        1ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ì„ ê³ ë„í™”ëœ êµ¬ì¡°ë¡œ ë¶„ì„í•˜ê³  ë¼ìš°íŒ… ê²°ì •ì„ ë‚´ë¦½ë‹ˆë‹¤.
        
        Args:
            state (ShoppingAgentState): í˜„ì¬ ì—ì´ì „íŠ¸ ìƒíƒœ
            
        Returns:
            ShoppingAgentState: ë¶„ì„ ê²°ê³¼ ë° ë¼ìš°íŒ… ê²°ì •ì´ ì¶”ê°€ëœ ìƒíƒœ
            
        Process:
            1. ê³ ë„í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ 3ë‹¨ê³„ ë¶„ì„ (ê¸°ë³¸ë¶„ì„, ë¼ìš°íŒ…ì „ëµ, ì‹¤í–‰ê³„íš)
            2. ë³µì¡ë„ ìˆ˜ì¤€ì— ë”°ë¥¸ ë¼ìš°íŒ… ê²°ì •
            3. ì‹¤í–‰ ê³„íš ê¸°ë°˜ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
            
        Key Output:
            - analyzed_query: 3ë‹¨ê³„ êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼
            - routing_decision: ë¼ìš°íŒ… ê²°ì • (simple_search/detailed_search/comprehensive_search)
            - search_keywords: ì‹¤í–‰ ê³„íš ê¸°ë°˜ í‚¤ì›Œë“œ
        """
        print("\n=== [1/4] ê³ ë„í™”ëœ ì§ˆë¬¸ ë¶„ì„ ë…¸ë“œ ì‹œì‘ ===")
        user_query = state["user_query"]
        print(f"ğŸ¯ ë¶„ì„í•  ì§ˆë¬¸: {user_query}")
        
        try:
            # í…œí”Œë¦¿ì—ì„œ {user_query} í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì•ˆì „í•˜ê²Œ ì¹˜í™˜
            analysis_prompt = self.analysis_prompt_template.replace("{user_query}", user_query)
            # Function calling ë°©ì‹ìœ¼ë¡œ structured output ì‚¬ìš©
            structured_llm = self.llm.with_structured_output(QueryAnalysis, method="function_calling")
            analysis_result = await structured_llm.ainvoke([HumanMessage(content=analysis_prompt)])
            
            # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜
            analyzed_data = analysis_result.model_dump()
            
            state["analyzed_query"] = analyzed_data
            
            # ê¸°ë³¸ ë¶„ì„ì—ì„œ ì •ë³´ ì¶”ì¶œ
            basic_analysis = analyzed_data.get("basic_analysis", {})
            routing_strategy = analyzed_data.get("routing_strategy", {})
            execution_plan = analyzed_data.get("execution_plan", {})
            
            state["search_keywords"] = basic_analysis.get("search_keywords", [])
            state["target_categories"] = basic_analysis.get("target_categories", [])
            state["routing_decision"] = routing_strategy.get("routing_decision", "detailed_search")
            state["processing_status"] = "ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ"
            
            print(f"âœ… ê³ ë„í™”ëœ ë¶„ì„ ì™„ë£Œ:")
            print(f"   - ì£¼ìš” ìƒí’ˆ: {basic_analysis.get('main_product')}")
            print(f"   - ê°€ê²©ëŒ€: {basic_analysis.get('price_range')}")
            print(f"   - ê²€ìƒ‰ í‚¤ì›Œë“œ: {basic_analysis.get('search_keywords')}")
            print(f"   - ë³µì¡ë„ ìˆ˜ì¤€: {routing_strategy.get('complexity_level')}")
            print(f"   - ë¼ìš°íŒ… ê²°ì •: {routing_strategy.get('routing_decision')}")
            print(f"   - ì£¼ìš” ê²€ìƒ‰ ì¿¼ë¦¬: {execution_plan.get('primary_search_query')}")
            
        except Exception as e:
            import traceback
            print(f"âŒ ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            print(f"ğŸ› ìƒì„¸ íŠ¸ë ˆì´ìŠ¤ë°±:\n{traceback.format_exc()}")
            state["error_info"] = f"ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}\n{traceback.format_exc()}"
            state["processing_status"] = "ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨"
            # ê¸°ë³¸ê°’ ì„¤ì •
            state["search_keywords"] = [user_query]
            state["target_categories"] = ["ì¼ë°˜"]
            state["routing_decision"] = "detailed_search"  # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¼ìš°íŒ…
            print(f"ğŸ”§ ê¸°ë³¸ê°’ ì„¤ì •: í‚¤ì›Œë“œ=[{user_query}], ë¼ìš°íŒ…=detailed_search")
            
        return state
    
    async def simple_search(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """
        ë‹¨ìˆœ ê²€ìƒ‰ ê²½ë¡œ: ê¸°ë³¸ ì›¹ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
        
        - ëª…í™•í•œ ë‹¨ì¼ ìƒí’ˆ ê²€ìƒ‰
        - ê¸°ë³¸ì ì¸ ì •ë³´ë§Œ í•„ìš”
        - ë¸Œëœë“œëª…ì´ë‚˜ êµ¬ì²´ì  ëª¨ë¸ëª… í¬í•¨ëœ ì§ˆë¬¸
        """
        print("\n=== [2/4] ë‹¨ìˆœ ê²€ìƒ‰ ë…¸ë“œ ì‹œì‘ ===")
        search_keywords = state["search_keywords"]
        analyzed_query = state.get("analyzed_query", {})
        execution_plan = analyzed_query.get("execution_plan", {})
        
        # ì£¼ìš” ê²€ìƒ‰ ì¿¼ë¦¬ ì‚¬ìš©
        primary_query = execution_plan.get("primary_search_query", " ".join(search_keywords[:2]))
        print(f"ğŸ” ì£¼ìš” ê²€ìƒ‰ ì¿¼ë¦¬: {primary_query}")
        
        try:
            # ë‹¨ìˆœ ê²€ìƒ‰ - ìµœì†Œí•œì˜ ê²°ê³¼ë§Œ ìˆ˜ì§‘
            tavily_tool = self.tools["tavily_search_tool"]
            response = tavily_tool.invoke({
                "query": primary_query,
                "search_depth": "basic",
                "max_results": 5  # ë‹¨ìˆœ ê²€ìƒ‰ì€ 5ê°œë§Œ
            })
            
            search_results = []
            relevant_urls = []
            
            for result in response.get("results", []):
                search_results.append({
                    "keyword": primary_query,
                    "title": result.get("title"),
                    "url": result.get("url"),
                    "content": result.get("content"),
                    "score": result.get("score", 0),
                    "relevance_score": self._calculate_relevance_score(result, primary_query)
                })
                
                if result.get("url"):
                    relevant_urls.append(result["url"])
            
            state["search_results"] = search_results
            state["relevant_urls"] = relevant_urls
            state["scraped_content"] = {}  # ë‹¨ìˆœ ê²€ìƒ‰ì€ ìŠ¤í¬ë˜í•‘ ì—†ìŒ
            state["product_data"] = []
            state["processing_status"] = f"ë‹¨ìˆœ ê²€ìƒ‰ ì™„ë£Œ ({len(search_results)}ê°œ ê²°ê³¼)"
            
            print(f"âœ… ë‹¨ìˆœ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            
        except Exception as e:
            print(f"âŒ ë‹¨ìˆœ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            state["error_info"] = f"ë‹¨ìˆœ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"
            state["processing_status"] = "ë‹¨ìˆœ ê²€ìƒ‰ ì‹¤íŒ¨"
            state["search_results"] = []
            state["relevant_urls"] = []
            
        return state
    
    async def detailed_search(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """
        ìƒì„¸ ê²€ìƒ‰ ê²½ë¡œ: ì›¹ ê²€ìƒ‰ + ì„ ë³„ì  ìŠ¤í¬ë˜í•‘
        
        - ì¼ë°˜ì ì¸ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
        - 2-3ê°œ ì¡°ê±´ ì¡°í•©
        - ë¸Œëœë“œ ë¹„êµë‚˜ ê¸°ë³¸ ìŠ¤í™ ë¹„êµ í•„ìš”
        """
        print("\n=== [2/4] ìƒì„¸ ê²€ìƒ‰ ë…¸ë“œ ì‹œì‘ ===")
        
        # ê¸°ì¡´ pre_searchì™€ pre_scrape ë¡œì§ì„ í†µí•©í•˜ì—¬ ì‹¤í–‰
        state = await self.pre_search(state)
        if state.get("search_results"):
            state = await self.pre_scrape(state)
        
        state["processing_status"] = "ìƒì„¸ ê²€ìƒ‰ ì™„ë£Œ"
        return state
    
    async def comprehensive_search(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """
        ì¢…í•© ê²€ìƒ‰ ê²½ë¡œ: ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ìƒ‰ + ì „ë©´ì  ìŠ¤í¬ë˜í•‘
        
        - ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
        - ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë¶„ì„ í•„ìš”
        - ì‹œì¥ ë™í–¥ì´ë‚˜ íŠ¸ë Œë“œ ë¶„ì„ í¬í•¨
        """
        print("\n=== [2/4] ì¢…í•© ê²€ìƒ‰ ë…¸ë“œ ì‹œì‘ ===")
        search_keywords = state["search_keywords"]
        analyzed_query = state.get("analyzed_query", {})
        execution_plan = analyzed_query.get("execution_plan", {})
        
        # ë³´ì¡° ê²€ìƒ‰ ì¿¼ë¦¬ë„ í¬í•¨í•˜ì—¬ í™•ì¥ ê²€ìƒ‰
        primary_query = execution_plan.get("primary_search_query", " ".join(search_keywords[:2]))
        secondary_queries = execution_plan.get("secondary_search_queries", search_keywords[2:])
        
        all_queries = [primary_query] + secondary_queries[:2]  # ìµœëŒ€ 3ê°œ ì¿¼ë¦¬
        print(f"ğŸ” ì¢…í•© ê²€ìƒ‰ ì¿¼ë¦¬: {all_queries}")
        
        try:
            search_results = []
            relevant_urls = []
            
            # ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰
            for query in all_queries:
                tavily_tool = self.tools["tavily_search_tool"]
                response = tavily_tool.invoke({
                    "query": query,
                    "search_depth": "advanced",
                    "max_results": 8  # ì¢…í•© ê²€ìƒ‰ì€ ë” ë§ì€ ê²°ê³¼
                })
                
                for result in response.get("results", []):
                    search_results.append({
                        "keyword": query,
                        "title": result.get("title"),
                        "url": result.get("url"),
                        "content": result.get("content"),
                        "score": result.get("score", 0),
                        "relevance_score": self._calculate_relevance_score(result, query)
                    })
                    
                    if result.get("url"):
                        relevant_urls.append(result["url"])
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            relevant_urls = list(set(relevant_urls))
            search_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
            
            state["search_results"] = search_results
            state["relevant_urls"] = relevant_urls
            
            # í™•ì¥ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
            if relevant_urls:
                state = await self.comprehensive_scrape(state)
            
            state["processing_status"] = f"ì¢…í•© ê²€ìƒ‰ ì™„ë£Œ ({len(search_results)}ê°œ ê²°ê³¼)"
            print(f"âœ… ì¢…í•© ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            
        except Exception as e:
            print(f"âŒ ì¢…í•© ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            state["error_info"] = f"ì¢…í•© ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"
            state["processing_status"] = "ì¢…í•© ê²€ìƒ‰ ì‹¤íŒ¨"
            state["search_results"] = []
            state["relevant_urls"] = []
            
        return state
    
    async def comprehensive_scrape(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """ì¢…í•© ê²€ìƒ‰ìš© í™•ì¥ ìŠ¤í¬ë˜í•‘"""
        print("\n=== ì¢…í•© ìŠ¤í¬ë˜í•‘ ì‹œì‘ ===")
        relevant_urls = state["relevant_urls"]
        search_results = state.get("search_results", [])
        
        # ë” ë§ì€ URL ìŠ¤í¬ë˜í•‘ (ìµœëŒ€ 8ê°œ)
        max_urls = min(8, len(relevant_urls))
        best_urls = self._select_best_urls_for_scraping(relevant_urls, search_results, max_urls)
        
        scraped_content = {}
        product_data = []
        
        for url in best_urls:
            try:
                firecrawl_tool = self.tools["firecrawl_scrape_tool"]
                scrape_result = firecrawl_tool.invoke({
                    "url": url,
                    "content_max_length": self.config.scraping.content_max_length * 2  # ë” ê¸´ ì½˜í…ì¸ 
                })
                
                if scrape_result.get("success"):
                    content = scrape_result["content"]
                    title = scrape_result["title"]
                    
                    scraped_content[url] = {
                        "title": title,
                        "content": content,
                        "timestamp": datetime.now().isoformat(),
                        "content_length": scrape_result["content_length"],
                        "content_truncated": scrape_result.get("content_truncated", False)
                    }
                    
                    extracted_product = self._extract_product_info(content, url)
                    if extracted_product:
                        product_data.append(extracted_product)
                        
            except Exception as e:
                print(f"âŒ URL ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ {url}: {str(e)}")
        
        state["scraped_content"] = scraped_content
        state["product_data"] = product_data
        
        print(f"âœ… ì¢…í•© ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(scraped_content)}ê°œ URL, {len(product_data)}ê°œ ìƒí’ˆ")
        return state
    
    async def pre_search(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """
        2ë‹¨ê³„: Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Args:
            state (ShoppingAgentState): ë¶„ì„ëœ ì§ˆë¬¸ ì •ë³´ê°€ í¬í•¨ëœ ìƒíƒœ
            
        Returns:
            ShoppingAgentState: ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
            
        Process:
            1. ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
            2. ì„¤ì •ì— ë”°ë¥¸ ê²€ìƒ‰ ê°œìˆ˜ ì œí•œ (API í˜¸ì¶œ ìµœì í™”)
            3. ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ë° ê²°ê³¼ ì •ë ¬
            4. ì‡¼í•‘ëª° ë„ë©”ì¸ ìš°ì„ ìˆœìœ„ ì ìš©
            
        Key Output:
            - search_results: ê´€ë ¨ì„± ì ìˆ˜ê°€ í¬í•¨ëœ ê²€ìƒ‰ ê²°ê³¼
            - relevant_urls: ë‹¤ìŒ ë‹¨ê³„ ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ URL ëª©ë¡
            
        Note:
            ê²€ìƒ‰ í’ˆì§ˆì´ ìµœì¢… ì¶”ì²œ í’ˆì§ˆì— ì§ì ‘ì  ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•µì‹¬ ë‹¨ê³„
        """
        print("\n=== [2/4] ì‚¬ì „ ê²€ìƒ‰ ë…¸ë“œ ì‹œì‘ ===")
        search_keywords = state["search_keywords"]
        print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")
        search_results = []
        relevant_urls = []
        
        try:
            # ì„¤ì •ì— ë”°ë¥¸ ê²€ìƒ‰ ê°œìˆ˜ ì œí•œ
            max_keywords = self.config.search.max_keywords_to_search
            max_results_per_keyword = self.config.search.max_results_per_keyword
            total_max_results = self.config.search.total_max_search_results
            
            # í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ì •ë ¬ (ê¸¸ì´ê°€ ì ë‹¹í•˜ê³  êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ìš°ì„ )
            sorted_keywords = sorted(search_keywords, key=lambda x: (len(x.split()), -len(x)))
            
            total_results_count = 0
            
            # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ìˆ˜í–‰ (ì œí•œëœ ê°œìˆ˜)
            for keyword in sorted_keywords[:max_keywords]:
                if total_results_count >= total_max_results:
                    break
                    
                # ì‡¼í•‘ í‚¤ì›Œë“œ ì¶”ê°€ (ì„¤ì •ì— ë”°ë¼)
                if self.config.search.add_shopping_keywords:
                    search_query = f"{keyword} ì‡¼í•‘ êµ¬ë§¤ ì¶”ì²œ"
                else:
                    search_query = keyword
                
                remaining_slots = min(
                    max_results_per_keyword, 
                    total_max_results - total_results_count
                )
                
                if remaining_slots <= 0:
                    break
                
                # LangChain ë„êµ¬ë¥¼ í†µí•œ Tavily ê²€ìƒ‰ ìˆ˜í–‰
                # ì´ ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œí•˜ë©´ ìë™ìœ¼ë¡œ on_tool_start/end ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ì—¬
                # UIì—ì„œ ë„êµ¬ ì‹¤í–‰ ìƒíƒœë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
                tavily_tool = self.tools["tavily_search_tool"]
                response = tavily_tool.invoke({
                    "query": search_query,
                    "search_depth": self.config.search.search_depth,
                    "max_results": remaining_slots
                })
                
                for result in response.get("results", []):
                    if total_results_count >= total_max_results:
                        break
                        
                    search_results.append({
                        "keyword": keyword,
                        "title": result.get("title"),
                        "url": result.get("url"),
                        "content": result.get("content"),
                        "score": result.get("score", 0),
                        "relevance_score": self._calculate_relevance_score(result, keyword)
                    })
                    
                    if result.get("url"):
                        relevant_urls.append(result["url"])
                        
                    total_results_count += 1
            
            # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
            search_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
            
            state["search_results"] = search_results
            state["relevant_urls"] = list(set(relevant_urls))  # ì¤‘ë³µ ì œê±°
            state["processing_status"] = f"ì‚¬ì „ ê²€ìƒ‰ ì™„ë£Œ ({len(search_results)}ê°œ ê²°ê³¼)"
            
            print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼, {len(relevant_urls)}ê°œ URL ë°œê²¬")
            if search_results:
                print(f"   - ìµœê³  ì ìˆ˜ ê²°ê³¼: {search_results[0]['title'][:50]}...")
            
            # ë°œê²¬ëœ URL ë¦¬ìŠ¤íŠ¸ í‘œì‹œ
            if relevant_urls:
                print(f"ğŸ”— ë°œê²¬ëœ URL ëª©ë¡:")
                for i, url in enumerate(relevant_urls[:5], 1):  # ìµœëŒ€ 5ê°œê¹Œì§€ í‘œì‹œ
                    print(f"   {i}. {url}")
                if len(relevant_urls) > 5:
                    print(f"   ... ì™¸ {len(relevant_urls) - 5}ê°œ URL")
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            state["error_info"] = f"ì‚¬ì „ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"
            state["processing_status"] = "ì‚¬ì „ ê²€ìƒ‰ ì‹¤íŒ¨"
            state["search_results"] = []
            state["relevant_urls"] = []
            
        return state
    
    def _calculate_relevance_score(self, result: Dict[str, Any], keyword: str) -> float:
        """
        ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            result (Dict[str, Any]): Tavily ê²€ìƒ‰ ê²°ê³¼ ê°ì²´
            keyword (str): ê²€ìƒ‰ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œ
            
        Returns:
            float: ê³„ì‚°ëœ ê´€ë ¨ì„± ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ê´€ë ¨ì„±ì´ ë†’ìŒ)
            
        Note:
            ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ëœ ì ìˆ˜ ê³„ì‚°
        """
        return calculate_relevance_score(result, keyword)
    
    async def pre_scrape(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """
        3ë‹¨ê³„: Firecrawl APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ ë³„ëœ URLì˜ ìƒì„¸ ì½˜í…ì¸ ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Args:
            state (ShoppingAgentState): ê²€ìƒ‰ ê²°ê³¼ì™€ URL ëª©ë¡ì´ í¬í•¨ëœ ìƒíƒœ
            
        Returns:
            ShoppingAgentState: ìŠ¤í¬ë˜í•‘ëœ ì½˜í…ì¸ ì™€ ìƒí’ˆ ì •ë³´ê°€ ì¶”ê°€ëœ ìƒíƒœ
            
        Process:
            1. ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ URL ì„ íƒ
            2. Firecrawlì„ í†µí•œ êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ì¶”ì¶œ
            3. ìƒí’ˆ ì •ë³´ ìë™ ì¶”ì¶œ (ì œëª©, ê°€ê²©, ì„¤ëª… ë“±)
            4. ì½˜í…ì¸  ê¸¸ì´ ì œí•œ ë° ì •ì œ
            
        Key Output:
            - scraped_content: URLë³„ ìŠ¤í¬ë˜í•‘ëœ ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
            - product_data: ì¶”ì¶œëœ ìƒí’ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            
        Note:
            ìµœì¢… ë‹µë³€ì˜ êµ¬ì²´ì„±ê³¼ ì •í™•ì„±ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ë‹¨ê³„
        """
        print("\n=== [3/4] ì‚¬ì „ ìŠ¤í¬ë˜í•‘ ë…¸ë“œ ì‹œì‘ ===")
        relevant_urls = state["relevant_urls"]
        search_results = state.get("search_results", [])
        print(f"ğŸ”— ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ URL: {len(relevant_urls)}ê°œ")
        scraped_content = {}
        product_data = []
        
        try:
            # ìŠ¤í¬ë˜í•‘í•  URL ê°œìˆ˜ ì œí•œ
            max_urls_to_scrape = self.config.scraping.max_urls_to_scrape
            
            if not relevant_urls:
                print("âš ï¸ ìŠ¤í¬ë˜í•‘í•  URLì´ ì—†ìŠµë‹ˆë‹¤")
                state["scraped_content"] = {}
                state["product_data"] = []
                state["processing_status"] = "ìŠ¤í¬ë˜í•‘í•  URL ì—†ìŒ"
                return state
            
            # ìµœì ì˜ URL ì„ íƒ
            best_urls = self._select_best_urls_for_scraping(relevant_urls, search_results, max_urls_to_scrape)
            print(f"ğŸ¯ ì„ íƒëœ ìµœì  URL: {len(best_urls)}ê°œ")
            
            # ì„ íƒëœ URL ìƒì„¸ í‘œì‹œ
            if best_urls:
                print("ğŸ“‹ ìŠ¤í¬ë˜í•‘ ì˜ˆì • URL:")
                for i, url in enumerate(best_urls, 1):
                    print(f"   {i}. {url}")
            else:
                print("âš ï¸ ìŠ¤í¬ë˜í•‘í•  URLì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            if best_urls:
                # Firecrawl ì§ì ‘ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                for url in best_urls:
                    try:
                        print(f"ğŸ“„ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
                        
                        # LangChain ë„êµ¬ë¥¼ í†µí•œ Firecrawl ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰
                        # ë„êµ¬ í˜¸ì¶œ ì‹œ ìë™ìœ¼ë¡œ on_tool_start/end ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ì—¬
                        # UIì—ì„œ ê° URLë³„ ìŠ¤í¬ë˜í•‘ ì§„í–‰ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
                        firecrawl_tool = self.tools["firecrawl_scrape_tool"]
                        scrape_result = firecrawl_tool.invoke({
                            "url": url,
                            "content_max_length": self.config.scraping.content_max_length
                        })
                        
                        if scrape_result.get("success"):
                            # ì„±ê³µì ì¸ ìŠ¤í¬ë˜í•‘
                            content = scrape_result["content"]
                            title = scrape_result["title"]
                            
                            scraped_content[url] = {
                                "title": title,
                                "content": content,
                                "timestamp": datetime.now().isoformat(),
                                "content_length": scrape_result["content_length"],
                                "content_truncated": scrape_result.get("content_truncated", False),
                                "original_data": content
                            }
                            
                            # ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ
                            extracted_product = self._extract_product_info(content, url)
                            if extracted_product:
                                product_data.append(extracted_product)
                            
                        else:
                            # ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨
                            error_msg = scrape_result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                            scraped_content[url] = {
                                "title": "ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨",
                                "content": f"ì˜¤ë¥˜: {error_msg}",
                                "timestamp": datetime.now().isoformat(),
                                "error": True
                            }
                            
                    except Exception as url_error:
                        # ê°œë³„ URL ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨
                        scraped_content[url] = {
                            "title": "ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨",
                            "content": f"ì˜¤ë¥˜: {str(url_error)}",
                            "timestamp": datetime.now().isoformat(),
                            "error": True
                        }
            
            state["scraped_content"] = scraped_content
            state["product_data"] = product_data
            state["processing_status"] = f"ì‚¬ì „ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ({len(scraped_content)}ê°œ URL)"
            
            print(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(scraped_content)}ê°œ URL, {len(product_data)}ê°œ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ")
            if product_data:
                print(f"   - ìƒí’ˆ ì¶”ì¶œ ì˜ˆì‹œ: {product_data[0]['name'][:30]}...")
            
        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
            state["error_info"] = f"ì‚¬ì „ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}"
            state["processing_status"] = "ì‚¬ì „ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
            state["scraped_content"] = {}
            state["product_data"] = []
            
        return state
    
    def _select_best_urls_for_scraping(self, relevant_urls: List[str], search_results: List[Dict], max_count: int) -> List[str]:
        """ìŠ¤í¬ë˜í•‘ì„ ìœ„í•œ ìµœì ì˜ URL ì„ íƒ"""
        if not relevant_urls:
            return []
        
        # URLë³„ ì ìˆ˜ ê³„ì‚°
        url_scores = {}
        
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        for result in search_results:
            url = result.get("url")
            if url and url in relevant_urls:
                url_scores[url] = result.get("relevance_score", 0)
        
        # ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” URLì€ ê¸°ë³¸ ì ìˆ˜ ë¶€ì—¬
        for url in relevant_urls:
            if url not in url_scores:
                url_scores[url] = 0.0
        
        # ì‡¼í•‘ëª° ë„ë©”ì¸ ìš°ì„ ìˆœìœ„ ì¶”ê°€
        for url in url_scores:
            for domain in self.config.scraping.preferred_shopping_domains:
                if domain in url.lower():
                    url_scores[url] += 0.3  # ì‡¼í•‘ëª° ë„ë©”ì¸ ê°€ì‚°ì 
                    break
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ URL ì„ íƒ
        sorted_urls = sorted(url_scores.items(), key=lambda x: x[1], reverse=True)
        best_urls = [url for url, score in sorted_urls[:max_count]]
        
        return best_urls
    
    
    def _extract_title(self, content: str) -> str:
        """
        ì½˜í…ì¸ ì—ì„œ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            content (str): ë¶„ì„í•  ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
            
        Returns:
            str: ì¶”ì¶œëœ ì œëª©
            
        Note:
            ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¡œ ìœ„ì„í•˜ì—¬ ì¼ê´€ëœ ì œëª© ì¶”ì¶œ
        """
        return extract_title_from_content(content)
    
    def _extract_product_info(self, content: str, url: str) -> Optional[Dict[str, Any]]:
        """
        ì½˜í…ì¸ ì—ì„œ ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            content (str): ìŠ¤í¬ë˜í•‘ëœ ì½˜í…ì¸ 
            url (str): ìƒí’ˆ í˜ì´ì§€ URL
            
        Returns:
            Optional[Dict[str, Any]]: ì¶”ì¶œëœ ìƒí’ˆ ì •ë³´ ë˜ëŠ” None
            
        Note:
            ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œì¤€í™”ëœ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
        """
        return extract_product_info_from_content(content, url)
    
    async def call_agent(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """
        4ë‹¨ê³„: ìˆ˜ì§‘ëœ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì „ë¬¸ì ì¸ ì‡¼í•‘ ì¶”ì²œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            state (ShoppingAgentState): ëª¨ë“  ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ê°€ í¬í•¨ëœ ìƒíƒœ
            
        Returns:
            ShoppingAgentState: ìµœì¢… ë‹µë³€ì´ í¬í•¨ëœ ì™„ë£Œ ìƒíƒœ
            
        Process:
            1. ë¶„ì„ ê²°ê³¼, ê²€ìƒ‰ ê²°ê³¼, ìƒí’ˆ ì •ë³´ë¥¼ í†µí•©ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
            2. ì „ë¬¸ ì‡¼í•‘ ì»¨ì„¤í„´íŠ¸ í˜ë¥´ì†Œë‚˜ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ ìƒì„±
            3. ê°œì¸í™”ëœ ì¶”ì²œ, ê°€ê²©ëŒ€ë³„ ì˜µì…˜, êµ¬ë§¤ ê°€ì´ë“œ í¬í•¨
            4. ì¥ë‹¨ì  ë¶„ì„ê³¼ ëŒ€ì•ˆ ìƒí’ˆ ì œì‹œë¡œ ì‹ ë¢°ì„± í™•ë³´
            
        Key Output:
            - final_answer: ì™„ì„±ëœ ì‡¼í•‘ ì¶”ì²œ ë‹µë³€
            - enriched_context: ìƒì„±ì— ì‚¬ìš©ëœ í†µí•© ì»¨í…ìŠ¤íŠ¸
            
        Note:
            ëª¨ë“  ì´ì „ ë‹¨ê³„ì˜ ì„±ê³¼ê°€ ì§‘ì•½ë˜ëŠ” ìµœì¢… ë‹¨ê³„
        """
        print("\n=== [4/4] React Agent ë…¸ë“œ ì‹œì‘ ===")
        print("ğŸ¤– ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        
        # ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼
        if state.get("analyzed_query"):
            context_parts.append(f"ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼: {json.dumps(state['analyzed_query'], ensure_ascii=False, indent=2)}")
        
        # ê²€ìƒ‰ ê²°ê³¼
        if state.get("search_results"):
            search_summary = []
            for result in state["search_results"][:10]:  # ìƒìœ„ 10ê°œ
                search_summary.append(f"- {result['title']}: {result['content'][:200]}...")
            context_parts.append(f"ê²€ìƒ‰ ê²°ê³¼:\n" + "\n".join(search_summary))
        
        # ìƒí’ˆ ë°ì´í„°
        if state.get("product_data"):
            product_summary = []
            for product in state["product_data"][:5]:  # ìƒìœ„ 5ê°œ
                product_summary.append(f"- {product['name']}: {product['price']} ({product['url']})")
            context_parts.append(f"ìˆ˜ì§‘ëœ ìƒí’ˆ ì •ë³´:\n" + "\n".join(product_summary))
        
        enriched_context = "\n\n".join(context_parts)
        state["enriched_context"] = enriched_context
        
        # React Agent í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = self.response_prompt_template
        
        try:
            # í…œí”Œë¦¿ì—ì„œ {context} í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì•ˆì „í•˜ê²Œ ì¹˜í™˜
            formatted_system_prompt = system_prompt.replace("{context}", enriched_context)
            messages = [
                SystemMessage(content=formatted_system_prompt),
                HumanMessage(content=state["user_query"])
            ]
            
            response = await self.llm.ainvoke(messages)
            state["final_answer"] = response.content
            state["processing_status"] = "ì²˜ë¦¬ ì™„ë£Œ"
            
            print(f"âœ… ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ ({len(response.content)}ì)")
            print(f"   - ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {response.content[:100]}...")
            
            # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            state["messages"].extend([
                HumanMessage(content=state["user_query"]),
                response
            ])
            
        except Exception as e:
            print(f"âŒ React Agent ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            state["error_info"] = f"React Agent ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
            state["processing_status"] = "ì²˜ë¦¬ ì‹¤íŒ¨"
            state["final_answer"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
        print("\n=== ğŸ‰ Enhanced Shopping Agent ì²˜ë¦¬ ì™„ë£Œ ===\n")
        return state


# ë„êµ¬ í•¨ìˆ˜ë“¤
@tool
def get_current_time() -> str:
    """í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


async def build_enhanced_agent(config_name: str = "credit_saving", prompt_name: str = "default") -> CompiledStateGraph:
    """Enhanced Shopping Agent ë¹Œë“œ"""
    config = get_config(config_name)
    agent = EnhancedShoppingAgent(config, prompt_name)
    return agent.create_workflow()


async def run_agent_test():
    """ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    agent = await build_enhanced_agent()
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_query = "ê²¨ìš¸ìš© ë”°ëœ»í•œ íŒ¨ë”© ì í¼ ì¶”ì²œí•´ì¤˜. 10ë§Œì› ì´í•˜ë¡œ ê²€ì€ìƒ‰ì´ë©´ ì¢‹ê² ì–´."
    
    initial_state = {
        "user_query": test_query,
        "messages": [],
        "processing_status": "ì‹œì‘"
    }
    
    print("=== Enhanced Shopping Agent í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    print(f"ì§ˆë¬¸: {test_query}")
    print()
    
    # ì—ì´ì „íŠ¸ ì‹¤í–‰
    async for chunk in agent.astream(
        initial_state,
        stream_mode="values"
    ):
        messages = chunk["messages"]

        for msg in messages:
            msg.pretty_print()

    # result = await agent.ainvoke(initial_state)
    
    # # ê²°ê³¼ ì¶œë ¥
    # print(f"ì²˜ë¦¬ ìƒíƒœ: {result.get('processing_status')}")
    # print(f"ìµœì¢… ë‹µë³€: {result.get('final_answer')}")
    
    # if result.get('error_info'):
    #     print(f"ì˜¤ë¥˜ ì •ë³´: {result['error_info']}")
    
    # # ìƒì„¸ ì •ë³´ ì¶œë ¥
    # print("\n=== ì²˜ë¦¬ ê³¼ì • ìƒì„¸ ===")
    # if result.get('analyzed_query'):
    #     print(f"ì§ˆë¬¸ ë¶„ì„: {json.dumps(result['analyzed_query'], ensure_ascii=False, indent=2)}")
    
    # if result.get('search_results'):
    #     print(f"ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(result['search_results'])}")
    
    # if result.get('product_data'):
    #     print(f"ìˆ˜ì§‘ëœ ìƒí’ˆ ìˆ˜: {len(result['product_data'])}")


if __name__ == "__main__":
    asyncio.run(run_agent_test())