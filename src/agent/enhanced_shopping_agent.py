"""
Enhanced Shopping Agent with Pre-Search and Pre-Scraping
LangGraphë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ì—ì´ì „íŠ¸ + ì‚¬ì „ ê²€ìƒ‰/ìŠ¤í¬ë˜í•‘ íŒŒì´í”„ë¼ì¸
"""

import os
import asyncio
import json
from typing import Annotated, TypedDict, List, Dict, Any, Optional
from datetime import datetime

from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import create_react_agent

from tavily import TavilyClient
from firecrawl import FirecrawlApp
from dotenv import load_dotenv
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from config.agent_config import AgentConfig, get_config
from utils.text_processing import (
    extract_title_from_content,
    extract_product_info_from_content,
    calculate_relevance_score
)
from utils.retry_helper import retry_on_failure

load_dotenv()


class QueryAnalysis(BaseModel):
    """ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ ìœ„í•œ êµ¬ì¡°í™”ëœ ëª¨ë¸"""
    main_product: str = Field(description="ì£¼ìš” ìƒí’ˆ/ì¹´í…Œê³ ë¦¬")
    # specific_requirements: Dict[str, str] = Field(description="êµ¬ì²´ì  ìš”êµ¬ì‚¬í•­ (ìƒ‰ìƒ, í¬ê¸°, ë¸Œëœë“œ ë“±)")
    price_range: str = Field(description="ê°€ê²©ëŒ€ ì •ë³´")
    search_keywords: List[str] = Field(description="ê²€ìƒ‰ì— ì‚¬ìš©í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸", max_items=5)
    target_categories: List[str] = Field(description="ëŒ€ìƒ ì¹´í…Œê³ ë¦¬")
    search_intent: str = Field(description="ê²€ìƒ‰ ì˜ë„ (êµ¬ë§¤, ë¹„êµ, ì •ë³´ìˆ˜ì§‘ ë“±)")


class ShoppingAgentState(TypedDict):
    """Enhanced Shopping Agent ìƒíƒœ ê´€ë¦¬"""
    messages: Annotated[list[BaseMessage], add_messages]
    user_query: str
    
    # ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼
    analyzed_query: Dict[str, Any]
    search_keywords: List[str]
    target_categories: List[str]
    
    # ì‚¬ì „ ê²€ìƒ‰ ê²°ê³¼
    search_results: List[Dict[str, Any]]
    relevant_urls: List[str]
    
    # ì‚¬ì „ ìŠ¤í¬ë˜í•‘ ê²°ê³¼
    scraped_content: Dict[str, Any]
    product_data: List[Dict[str, Any]]
    
    # React Agent ì»¨í…ìŠ¤íŠ¸
    enriched_context: str
    
    # ìµœì¢… ê²°ê³¼
    final_answer: str
    processing_status: str
    error_info: Optional[str]


class EnhancedShoppingAgent:
    """
    í–¥ìƒëœ ì‡¼í•‘ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤
    
    LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¶•ëœ 4ë‹¨ê³„ ì‡¼í•‘ ì¶”ì²œ ì—ì´ì „íŠ¸:
    1. ì§ˆë¬¸ ë¶„ì„: ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ êµ¬ì¡°í™”ëœ ì •ë³´ë¡œ ë³€í™˜
    2. ì‚¬ì „ ê²€ìƒ‰: Tavilyë¥¼ í†µí•œ ê´€ë ¨ ì •ë³´ ìˆ˜ì§‘
    3. ì‚¬ì „ ìŠ¤í¬ë˜í•‘: Firecrawlì„ í†µí•œ ìƒì„¸ ì½˜í…ì¸  ìˆ˜ì§‘
    4. ìµœì¢… ë‹µë³€: ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì  ì¶”ì²œ ì œê³µ
    
    Features:
        - Structured outputì„ í†µí•œ ì•ˆì •ì ì¸ ì§ˆë¬¸ ë¶„ì„
        - ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ URL ì„ íƒ
        - ì„¤ì • ê¸°ë°˜ API í˜¸ì¶œ ìµœì í™”
        - ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™© ë¡œê¹…
    """
    
    def __init__(self, config: AgentConfig = None):
        """
        ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config (AgentConfig, optional): ì—ì´ì „íŠ¸ ì„¤ì • ê°ì²´. 
                                          Noneì¸ ê²½ìš° ê¸°ë³¸ ì„¤ì • ì‚¬ìš©.
        
        Note:
            - OpenAI, Tavily, Firecrawl API í‚¤ê°€ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨
            - ì„¤ì •ì„ í†µí•´ ê²€ìƒ‰/ìŠ¤í¬ë˜í•‘ ë²”ìœ„ ì¡°ì ˆ ê°€ëŠ¥
        """
        # ì„¤ì • ì´ˆê¸°í™” - ê¸°ë³¸ê°’ ë˜ëŠ” ì „ë‹¬ë°›ì€ ì„¤ì • ì‚¬ìš©
        self.config = config or get_config("default")
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (OpenAI GPT ëª¨ë¸)
        self.llm = ChatOpenAI(
            model=self.config.llm_model,
            temperature=self.config.llm_temperature,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # ì™¸ë¶€ ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
        self.firecrawl_client = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
        
        
    def create_workflow(self) -> CompiledStateGraph:
        """
        LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤.
        
        Returns:
            CompiledStateGraph: ì‹¤í–‰ ê°€ëŠ¥í•œ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„
            
        Workflow:
            analyze_query â†’ pre_search â†’ pre_scrape â†’ react_agent â†’ END
            
        Note:
            ê° ë…¸ë“œëŠ” ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ë©°, ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ í™œìš©
        """
        workflow = StateGraph(ShoppingAgentState)
        
        # ë…¸ë“œ ì¶”ê°€ - ê° ë‹¨ê³„ë³„ ì²˜ë¦¬ í•¨ìˆ˜ ì—°ê²°
        workflow.add_node("analyze_query", self.analyze_query)    # 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ì„
        workflow.add_node("pre_search", self.pre_search)          # 2ë‹¨ê³„: ì‚¬ì „ ê²€ìƒ‰
        workflow.add_node("pre_scrape", self.pre_scrape)          # 3ë‹¨ê³„: ì‚¬ì „ ìŠ¤í¬ë˜í•‘
        workflow.add_node("react_agent", self.call_agent)         # 4ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„±
        
        # ì›Œí¬í”Œë¡œìš° ê²½ë¡œ ì •ì˜ (ì„ í˜• ì‹¤í–‰)
        workflow.set_entry_point("analyze_query")
        workflow.add_edge("analyze_query", "pre_search")
        workflow.add_edge("pre_search", "pre_scrape")
        workflow.add_edge("pre_scrape", "react_agent")
        workflow.add_edge("react_agent", END)
        
        return workflow.compile()
    
    async def analyze_query(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """
        1ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ì„ êµ¬ì¡°í™”ëœ ì •ë³´ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            state (ShoppingAgentState): í˜„ì¬ ì—ì´ì „íŠ¸ ìƒíƒœ
            
        Returns:
            ShoppingAgentState: ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
            
        Process:
            1. Structured Outputì„ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ì¸ íŒŒì‹±
            2. ê²€ìƒ‰ í‚¤ì›Œë“œ, ìƒí’ˆ ì¹´í…Œê³ ë¦¬, ê°€ê²©ëŒ€ ë“± ì¶”ì¶œ
            3. ì‡¼í•‘ ì˜ë„ ë¶„ì„ (êµ¬ë§¤, ë¹„êµ, ì •ë³´ìˆ˜ì§‘ ë“±)
            
        Key Output:
            - analyzed_query: êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼
            - search_keywords: ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ê²€ìƒ‰ í‚¤ì›Œë“œ
            - target_categories: ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì •ë³´
        """
        print("\n=== [1/4] ì§ˆë¬¸ ë¶„ì„ ë…¸ë“œ ì‹œì‘ ===")
        user_query = state["user_query"]
        print(f"ğŸ¯ ë¶„ì„í•  ì§ˆë¬¸: {user_query}")
        
        analysis_prompt = f"""
        ë‹¹ì‹ ì€ ì „ë¬¸ ì‡¼í•‘ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì‡¼í•‘ ì§ˆë¬¸ì„ ì‹¬ì¸µ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ìƒí’ˆ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.

        ğŸ¯ **ì¤‘ìš”**: search_keywordsëŠ” ì´í›„ ì›¹ ê²€ìƒ‰ê³¼ ìƒí’ˆ ì¶”ì²œì˜ í•µì‹¬ì´ ë©ë‹ˆë‹¤. ë§¤ìš° ì‹ ì¤‘í•˜ê²Œ ì„ íƒí•˜ì„¸ìš”.

        **ì‚¬ìš©ì ì§ˆë¬¸**: "{user_query}"

        **ë¶„ì„ ì§€ì¹¨**:

        1. **main_product (ì£¼ìš” ìƒí’ˆ)**: 
           - ì‚¬ìš©ìê°€ ì°¾ëŠ” ì •í™•í•œ ìƒí’ˆëª…ì´ë‚˜ ì¹´í…Œê³ ë¦¬
           - ì˜ˆ: "íŒ¨ë”© ì í¼", "ë¬´ì„  ì´ì–´í°", "ìš´ë™í™”"

        2. **search_keywords (ê²€ìƒ‰ í‚¤ì›Œë“œ - ë§¤ìš° ì¤‘ìš”!)**: 
           âš ï¸ **ì´ í‚¤ì›Œë“œë“¤ì´ ê²€ìƒ‰ í’ˆì§ˆì„ ê²°ì •í•©ë‹ˆë‹¤!**
           
           **í¬í•¨í•´ì•¼ í•  í‚¤ì›Œë“œ ìœ í˜•:**
           - í•µì‹¬ ìƒí’ˆëª… (ì˜ˆ: "íŒ¨ë”©", "ì í¼", "ì½”íŠ¸")
           - êµ¬ì²´ì  íŠ¹ì§• (ì˜ˆ: "ë°©ìˆ˜", "ê²½ëŸ‰", "ì´ˆê²½ëŸ‰", "êµ¬ìŠ¤ë‹¤ìš´")
           - ë¸Œëœë“œëª… (ì–¸ê¸‰ëœ ê²½ìš°)
           - ìš©ë„/ì‹œì¦Œ (ì˜ˆ: "ê²¨ìš¸ìš©", "ë“±ì‚°ìš©", "ë°ì¼ë¦¬")
           - ì„±ë³„/ì—°ë ¹ (ì˜ˆ: "ë‚¨ì„±", "ì—¬ì„±", "ì•„ë™ìš©")
           - ê°€ê²©ëŒ€ í‚¤ì›Œë“œ (ì˜ˆ: "ì €ë ´í•œ", "í”„ë¦¬ë¯¸ì—„", "ê°€ì„±ë¹„")
           
           **í‚¤ì›Œë“œ ì„ íƒ ì›ì¹™:**
           - ê²€ìƒ‰ ê²°ê³¼ì˜ ì •í™•ì„±ì„ ë†’ì´ëŠ” í‚¤ì›Œë“œ ìš°ì„ 
           - ë„ˆë¬´ ì¼ë°˜ì ì´ì§€ ì•Šê³ , ë„ˆë¬´ êµ¬ì²´ì ì´ì§€ë„ ì•Šì€ ê· í˜•
           - ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì—ì„œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ê²€ìƒ‰ì–´
           - ìµœëŒ€ 5ê°œê¹Œì§€, ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ë°°ì—´
           
           **ì¢‹ì€ ì˜ˆì‹œ:**
           - "ê²¨ìš¸ íŒ¨ë”© ì¶”ì²œ" â†’ ["ê²¨ìš¸íŒ¨ë”©", "ë¡±íŒ¨ë”©", "ë‹¤ìš´ì¬í‚·", "ë°©í•œë³µ", "ì•„ìš°í„°"]
           - "ë¬´ì„  ì´ì–´í°" â†’ ["ë¬´ì„ ì´ì–´í°", "ë¸”ë£¨íˆ¬ìŠ¤ì´ì–´í°", "ì—ì–´íŒŸ", "TWSì´ì–´í°", "ë„¥ë°´ë“œ"]

        3. **price_range (ê°€ê²©ëŒ€)**:
           - êµ¬ì²´ì  ê¸ˆì•¡ì´ ì–¸ê¸‰ëœ ê²½ìš°: "10ë§Œì› ì´í•˜", "50-100ë§Œì›"
           - ì¶”ìƒì  í‘œí˜„ì˜ ê²½ìš°: "ì €ë ´í•œ", "ê°€ì„±ë¹„", "í”„ë¦¬ë¯¸ì—„"
           - ì–¸ê¸‰ ì—†ìœ¼ë©´: "ê°€ê²© ì •ë³´ ì—†ìŒ"

        4. **target_categories (ëŒ€ìƒ ì¹´í…Œê³ ë¦¬)**:
           - íŒ¨ì…˜, ì „ìì œí’ˆ, ìƒí™œìš©í’ˆ, ìŠ¤í¬ì¸ /ë ˆì €, ë·°í‹°, ê°€ì „, ìë™ì°¨, ë„ì„œ ë“±
           - ì£¼ ì¹´í…Œê³ ë¦¬ì™€ ì„œë¸Œ ì¹´í…Œê³ ë¦¬ í¬í•¨

        5. **search_intent (ê²€ìƒ‰ ì˜ë„)**:
           - "êµ¬ë§¤": ë°”ë¡œ êµ¬ë§¤í•˜ë ¤ëŠ” ì˜ë„
           - "ë¹„êµ": ì—¬ëŸ¬ ìƒí’ˆì„ ë¹„êµí•˜ë ¤ëŠ” ì˜ë„  
           - "ì •ë³´ìˆ˜ì§‘": ìƒí’ˆì— ëŒ€í•œ ì •ë³´ë¥¼ ì–»ìœ¼ë ¤ëŠ” ì˜ë„
           - "ì¶”ì²œ": ì¶”ì²œì„ ë°›ìœ¼ë ¤ëŠ” ì˜ë„

        **ë¶„ì„ ì‹œ ê³ ë ¤ì‚¬í•­**:
        - ì‚¬ìš©ìì˜ ì•”ë¬µì  ìš”êµ¬ì‚¬í•­ íŒŒì•… (ì˜ˆ: "íšŒì‚¬ì›" â†’ "ë¹„ì¦ˆë‹ˆìŠ¤ ìºì£¼ì–¼")
        - ê³„ì ˆì„± ê³ ë ¤ (ì˜ˆ: ê²¨ìš¸ â†’ ë°©í•œ ì œí’ˆ)
        - íŠ¸ë Œë“œ ë°˜ì˜ (ì˜ˆ: "MZì„¸ëŒ€ ì¸ê¸°" â†’ "íŠ¸ë Œë””í•œ")
        - ì‹¤ìš©ì„± vs ì‹¬ë¯¸ì„± ê· í˜•

        ìœ„ ì§€ì¹¨ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”.
        """
        
        try:
            # Function calling ë°©ì‹ìœ¼ë¡œ structured output ì‚¬ìš©
            structured_llm = self.llm.with_structured_output(QueryAnalysis, method="function_calling")
            analysis_result = await structured_llm.ainvoke([HumanMessage(content=analysis_prompt)])
            
            # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜
            analyzed_data = analysis_result.model_dump()
            
            state["analyzed_query"] = analyzed_data
            state["search_keywords"] = analyzed_data.get("search_keywords", [])
            state["target_categories"] = analyzed_data.get("target_categories", [])
            state["processing_status"] = "ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ"
            
            print(f"âœ… ë¶„ì„ ì™„ë£Œ:")
            print(f"   - ì£¼ìš” ìƒí’ˆ: {analyzed_data.get('main_product')}")
            print(f"   - ê°€ê²©ëŒ€: {analyzed_data.get('price_range')}")
            print(f"   - ê²€ìƒ‰ í‚¤ì›Œë“œ: {analyzed_data.get('search_keywords')}")
            print(f"   - ê²€ìƒ‰ ì˜ë„: {analyzed_data.get('search_intent')}")
            
        except Exception as e:
            print(f"âŒ ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            state["error_info"] = f"ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            state["processing_status"] = "ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨"
            # ê¸°ë³¸ê°’ ì„¤ì •
            state["search_keywords"] = [user_query]
            state["target_categories"] = ["ì¼ë°˜"]
            print(f"ğŸ”§ ê¸°ë³¸ê°’ ì„¤ì •: í‚¤ì›Œë“œ=[{user_query}], ì¹´í…Œê³ ë¦¬=[ì¼ë°˜]")
            
        return state
    
    async def pre_search(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """
        2ë‹¨ê³„: Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Args:
            state (ShoppingAgentState): ë¶„ì„ëœ ì§ˆë¬¸ ì •ë³´ê°€ í¬í•¨ëœ ìƒíƒœ
            
        Returns:
            ShoppingAgentState: ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
            
        Process:
            1. ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
            2. ì„¤ì •ì— ë”°ë¥¸ ê²€ìƒ‰ ê°œìˆ˜ ì œí•œ (API í˜¸ì¶œ ìµœì í™”)
            3. ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ë° ê²°ê³¼ ì •ë ¬
            4. ì‡¼í•‘ëª° ë„ë©”ì¸ ìš°ì„ ìˆœìœ„ ì ìš©
            
        Key Output:
            - search_results: ê´€ë ¨ì„± ì ìˆ˜ê°€ í¬í•¨ëœ ê²€ìƒ‰ ê²°ê³¼
            - relevant_urls: ë‹¤ìŒ ë‹¨ê³„ ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ URL ëª©ë¡
            
        Note:
            ê²€ìƒ‰ í’ˆì§ˆì´ ìµœì¢… ì¶”ì²œ í’ˆì§ˆì— ì§ì ‘ì  ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•µì‹¬ ë‹¨ê³„
        """
        print("\n=== [2/4] ì‚¬ì „ ê²€ìƒ‰ ë…¸ë“œ ì‹œì‘ ===")
        search_keywords = state["search_keywords"]
        print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")
        search_results = []
        relevant_urls = []
        
        try:
            # ì„¤ì •ì— ë”°ë¥¸ ê²€ìƒ‰ ê°œìˆ˜ ì œí•œ
            max_keywords = self.config.search.max_keywords_to_search
            max_results_per_keyword = self.config.search.max_results_per_keyword
            total_max_results = self.config.search.total_max_search_results
            
            # í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ì •ë ¬ (ê¸¸ì´ê°€ ì ë‹¹í•˜ê³  êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ìš°ì„ )
            sorted_keywords = sorted(search_keywords, key=lambda x: (len(x.split()), -len(x)))
            
            total_results_count = 0
            
            # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ìˆ˜í–‰ (ì œí•œëœ ê°œìˆ˜)
            for keyword in sorted_keywords[:max_keywords]:
                if total_results_count >= total_max_results:
                    break
                    
                # ì‡¼í•‘ í‚¤ì›Œë“œ ì¶”ê°€ (ì„¤ì •ì— ë”°ë¼)
                if self.config.search.add_shopping_keywords:
                    search_query = f"{keyword} ì‡¼í•‘ êµ¬ë§¤ ì¶”ì²œ"
                else:
                    search_query = keyword
                
                remaining_slots = min(
                    max_results_per_keyword, 
                    total_max_results - total_results_count
                )
                
                if remaining_slots <= 0:
                    break
                
                response = self.tavily_client.search(
                    query=search_query,
                    search_depth=self.config.search.search_depth,
                    max_results=remaining_slots
                )
                
                
                for result in response.get("results", []):
                    if total_results_count >= total_max_results:
                        break
                        
                    search_results.append({
                        "keyword": keyword,
                        "title": result.get("title"),
                        "url": result.get("url"),
                        "content": result.get("content"),
                        "score": result.get("score", 0),
                        "relevance_score": self._calculate_relevance_score(result, keyword)
                    })
                    
                    if result.get("url"):
                        relevant_urls.append(result["url"])
                        
                    total_results_count += 1
            
            # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
            search_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
            
            state["search_results"] = search_results
            state["relevant_urls"] = list(set(relevant_urls))  # ì¤‘ë³µ ì œê±°
            state["processing_status"] = f"ì‚¬ì „ ê²€ìƒ‰ ì™„ë£Œ ({len(search_results)}ê°œ ê²°ê³¼)"
            
            print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼, {len(relevant_urls)}ê°œ URL ë°œê²¬")
            if search_results:
                print(f"   - ìµœê³  ì ìˆ˜ ê²°ê³¼: {search_results[0]['title'][:50]}...")
            
            # ë°œê²¬ëœ URL ë¦¬ìŠ¤íŠ¸ í‘œì‹œ
            if relevant_urls:
                print(f"ğŸ”— ë°œê²¬ëœ URL ëª©ë¡:")
                for i, url in enumerate(relevant_urls[:5], 1):  # ìµœëŒ€ 5ê°œê¹Œì§€ í‘œì‹œ
                    print(f"   {i}. {url}")
                if len(relevant_urls) > 5:
                    print(f"   ... ì™¸ {len(relevant_urls) - 5}ê°œ URL")
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            state["error_info"] = f"ì‚¬ì „ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"
            state["processing_status"] = "ì‚¬ì „ ê²€ìƒ‰ ì‹¤íŒ¨"
            state["search_results"] = []
            state["relevant_urls"] = []
            
        return state
    
    def _calculate_relevance_score(self, result: Dict[str, Any], keyword: str) -> float:
        """
        ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            result (Dict[str, Any]): Tavily ê²€ìƒ‰ ê²°ê³¼ ê°ì²´
            keyword (str): ê²€ìƒ‰ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œ
            
        Returns:
            float: ê³„ì‚°ëœ ê´€ë ¨ì„± ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ê´€ë ¨ì„±ì´ ë†’ìŒ)
            
        Note:
            ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ëœ ì ìˆ˜ ê³„ì‚°
        """
        return calculate_relevance_score(result, keyword)
    
    async def pre_scrape(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """
        3ë‹¨ê³„: Firecrawl APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ ë³„ëœ URLì˜ ìƒì„¸ ì½˜í…ì¸ ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Args:
            state (ShoppingAgentState): ê²€ìƒ‰ ê²°ê³¼ì™€ URL ëª©ë¡ì´ í¬í•¨ëœ ìƒíƒœ
            
        Returns:
            ShoppingAgentState: ìŠ¤í¬ë˜í•‘ëœ ì½˜í…ì¸ ì™€ ìƒí’ˆ ì •ë³´ê°€ ì¶”ê°€ëœ ìƒíƒœ
            
        Process:
            1. ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ URL ì„ íƒ
            2. Firecrawlì„ í†µí•œ êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ì¶”ì¶œ
            3. ìƒí’ˆ ì •ë³´ ìë™ ì¶”ì¶œ (ì œëª©, ê°€ê²©, ì„¤ëª… ë“±)
            4. ì½˜í…ì¸  ê¸¸ì´ ì œí•œ ë° ì •ì œ
            
        Key Output:
            - scraped_content: URLë³„ ìŠ¤í¬ë˜í•‘ëœ ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
            - product_data: ì¶”ì¶œëœ ìƒí’ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            
        Note:
            ìµœì¢… ë‹µë³€ì˜ êµ¬ì²´ì„±ê³¼ ì •í™•ì„±ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ë‹¨ê³„
        """
        print("\n=== [3/4] ì‚¬ì „ ìŠ¤í¬ë˜í•‘ ë…¸ë“œ ì‹œì‘ ===")
        relevant_urls = state["relevant_urls"]
        search_results = state.get("search_results", [])
        print(f"ğŸ”— ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ URL: {len(relevant_urls)}ê°œ")
        scraped_content = {}
        product_data = []
        
        try:
            # ìŠ¤í¬ë˜í•‘í•  URL ê°œìˆ˜ ì œí•œ
            max_urls_to_scrape = self.config.scraping.max_urls_to_scrape
            
            if not relevant_urls:
                print("âš ï¸ ìŠ¤í¬ë˜í•‘í•  URLì´ ì—†ìŠµë‹ˆë‹¤")
                state["scraped_content"] = {}
                state["product_data"] = []
                state["processing_status"] = "ìŠ¤í¬ë˜í•‘í•  URL ì—†ìŒ"
                return state
            
            # ìµœì ì˜ URL ì„ íƒ
            best_urls = self._select_best_urls_for_scraping(relevant_urls, search_results, max_urls_to_scrape)
            print(f"ğŸ¯ ì„ íƒëœ ìµœì  URL: {len(best_urls)}ê°œ")
            
            # ì„ íƒëœ URL ìƒì„¸ í‘œì‹œ
            if best_urls:
                print("ğŸ“‹ ìŠ¤í¬ë˜í•‘ ì˜ˆì • URL:")
                for i, url in enumerate(best_urls, 1):
                    print(f"   {i}. {url}")
            else:
                print("âš ï¸ ìŠ¤í¬ë˜í•‘í•  URLì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            if best_urls:
                # Firecrawl ì§ì ‘ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                for url in best_urls:
                    try:
                        print(f"ğŸ“„ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
                        
                        # Firecrawlë¡œ ìŠ¤í¬ë˜í•‘ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨)
                        scrape_result = await self._scrape_url_with_retry(url)
                        
                        if scrape_result and scrape_result.success:
                            # content = scrape_result.get("data", {}).get("markdown", "")
                            content = scrape_result.markdown
                            
                            # ì½˜í…ì¸  ê¸¸ì´ ì œí•œ
                            content_limit = self.config.scraping.content_max_length
                            limited_content = content[:content_limit] if len(content) > content_limit else content
                            
                            scraped_content[url] = {
                                "title": self._extract_title(limited_content),
                                "content": limited_content,
                                "timestamp": datetime.now().isoformat(),
                                "content_length": len(content),
                                "content_truncated": len(content) > content_limit,
                                "original_data": content
                            }
                            
                            # ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ
                            extracted_product = self._extract_product_info(limited_content, url)
                            if extracted_product:
                                product_data.append(extracted_product)
                        else:
                            # ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨
                            error_msg = scrape_result.error if scrape_result else "ì‘ë‹µ ì—†ìŒ"
                            scraped_content[url] = {
                                "title": "ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨",
                                "content": f"ì˜¤ë¥˜: {error_msg}",
                                "timestamp": datetime.now().isoformat(),
                                "error": True
                            }
                            
                    except Exception as url_error:
                        # ê°œë³„ URL ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨
                        scraped_content[url] = {
                            "title": "ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨",
                            "content": f"ì˜¤ë¥˜: {str(url_error)}",
                            "timestamp": datetime.now().isoformat(),
                            "error": True
                        }
            
            state["scraped_content"] = scraped_content
            state["product_data"] = product_data
            state["processing_status"] = f"ì‚¬ì „ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ({len(scraped_content)}ê°œ URL)"
            
            print(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(scraped_content)}ê°œ URL, {len(product_data)}ê°œ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ")
            if product_data:
                print(f"   - ìƒí’ˆ ì¶”ì¶œ ì˜ˆì‹œ: {product_data[0]['name'][:30]}...")
            
        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
            state["error_info"] = f"ì‚¬ì „ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}"
            state["processing_status"] = "ì‚¬ì „ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
            state["scraped_content"] = {}
            state["product_data"] = []
            
        return state
    
    def _select_best_urls_for_scraping(self, relevant_urls: List[str], search_results: List[Dict], max_count: int) -> List[str]:
        """ìŠ¤í¬ë˜í•‘ì„ ìœ„í•œ ìµœì ì˜ URL ì„ íƒ"""
        if not relevant_urls:
            return []
        
        # URLë³„ ì ìˆ˜ ê³„ì‚°
        url_scores = {}
        
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        for result in search_results:
            url = result.get("url")
            if url and url in relevant_urls:
                url_scores[url] = result.get("relevance_score", 0)
        
        # ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” URLì€ ê¸°ë³¸ ì ìˆ˜ ë¶€ì—¬
        for url in relevant_urls:
            if url not in url_scores:
                url_scores[url] = 0.0
        
        # ì‡¼í•‘ëª° ë„ë©”ì¸ ìš°ì„ ìˆœìœ„ ì¶”ê°€
        for url in url_scores:
            for domain in self.config.scraping.preferred_shopping_domains:
                if domain in url.lower():
                    url_scores[url] += 0.3  # ì‡¼í•‘ëª° ë„ë©”ì¸ ê°€ì‚°ì 
                    break
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ URL ì„ íƒ
        sorted_urls = sorted(url_scores.items(), key=lambda x: x[1], reverse=True)
        best_urls = [url for url, score in sorted_urls[:max_count]]
        
        return best_urls
    
    @retry_on_failure(max_retries=2, delay=1.0)
    async def _scrape_url_with_retry(self, url: str):
        """
        ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì´ í¬í•¨ëœ URL ìŠ¤í¬ë˜í•‘
        
        Args:
            url (str): ìŠ¤í¬ë˜í•‘í•  URL
            
        Returns:
            Firecrawl ì‘ë‹µ ê°ì²´
            
        Note:
            502 ì—ëŸ¬ ë“± ì¼ì‹œì  ì¥ì• ì— ëŒ€ì‘í•˜ì—¬ ìµœëŒ€ 2íšŒ ì¬ì‹œë„
        """
        import asyncio
        
        # Firecrawlì€ ë™ê¸° APIì´ë¯€ë¡œ ë¹„ë™ê¸° ë˜í¼ ì‚¬ìš©
        loop = asyncio.get_event_loop()
        
        def sync_scrape():
            return self.firecrawl_client.scrape_url(url, formats=["markdown"])
        
        return await loop.run_in_executor(None, sync_scrape)
    
    def _extract_title(self, content: str) -> str:
        """
        ì½˜í…ì¸ ì—ì„œ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            content (str): ë¶„ì„í•  ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
            
        Returns:
            str: ì¶”ì¶œëœ ì œëª©
            
        Note:
            ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¡œ ìœ„ì„í•˜ì—¬ ì¼ê´€ëœ ì œëª© ì¶”ì¶œ
        """
        return extract_title_from_content(content)
    
    def _extract_product_info(self, content: str, url: str) -> Optional[Dict[str, Any]]:
        """
        ì½˜í…ì¸ ì—ì„œ ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            content (str): ìŠ¤í¬ë˜í•‘ëœ ì½˜í…ì¸ 
            url (str): ìƒí’ˆ í˜ì´ì§€ URL
            
        Returns:
            Optional[Dict[str, Any]]: ì¶”ì¶œëœ ìƒí’ˆ ì •ë³´ ë˜ëŠ” None
            
        Note:
            ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œì¤€í™”ëœ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
        """
        return extract_product_info_from_content(content, url)
    
    async def call_agent(self, state: ShoppingAgentState) -> ShoppingAgentState:
        """
        4ë‹¨ê³„: ìˆ˜ì§‘ëœ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì „ë¬¸ì ì¸ ì‡¼í•‘ ì¶”ì²œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            state (ShoppingAgentState): ëª¨ë“  ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ê°€ í¬í•¨ëœ ìƒíƒœ
            
        Returns:
            ShoppingAgentState: ìµœì¢… ë‹µë³€ì´ í¬í•¨ëœ ì™„ë£Œ ìƒíƒœ
            
        Process:
            1. ë¶„ì„ ê²°ê³¼, ê²€ìƒ‰ ê²°ê³¼, ìƒí’ˆ ì •ë³´ë¥¼ í†µí•©ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
            2. ì „ë¬¸ ì‡¼í•‘ ì»¨ì„¤í„´íŠ¸ í˜ë¥´ì†Œë‚˜ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ ìƒì„±
            3. ê°œì¸í™”ëœ ì¶”ì²œ, ê°€ê²©ëŒ€ë³„ ì˜µì…˜, êµ¬ë§¤ ê°€ì´ë“œ í¬í•¨
            4. ì¥ë‹¨ì  ë¶„ì„ê³¼ ëŒ€ì•ˆ ìƒí’ˆ ì œì‹œë¡œ ì‹ ë¢°ì„± í™•ë³´
            
        Key Output:
            - final_answer: ì™„ì„±ëœ ì‡¼í•‘ ì¶”ì²œ ë‹µë³€
            - enriched_context: ìƒì„±ì— ì‚¬ìš©ëœ í†µí•© ì»¨í…ìŠ¤íŠ¸
            
        Note:
            ëª¨ë“  ì´ì „ ë‹¨ê³„ì˜ ì„±ê³¼ê°€ ì§‘ì•½ë˜ëŠ” ìµœì¢… ë‹¨ê³„
        """
        print("\n=== [4/4] React Agent ë…¸ë“œ ì‹œì‘ ===")
        print("ğŸ¤– ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        
        # ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼
        if state.get("analyzed_query"):
            context_parts.append(f"ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼: {json.dumps(state['analyzed_query'], ensure_ascii=False, indent=2)}")
        
        # ê²€ìƒ‰ ê²°ê³¼
        if state.get("search_results"):
            search_summary = []
            for result in state["search_results"][:10]:  # ìƒìœ„ 10ê°œ
                search_summary.append(f"- {result['title']}: {result['content'][:200]}...")
            context_parts.append(f"ê²€ìƒ‰ ê²°ê³¼:\n" + "\n".join(search_summary))
        
        # ìƒí’ˆ ë°ì´í„°
        if state.get("product_data"):
            product_summary = []
            for product in state["product_data"][:5]:  # ìƒìœ„ 5ê°œ
                product_summary.append(f"- {product['name']}: {product['price']} ({product['url']})")
            context_parts.append(f"ìˆ˜ì§‘ëœ ìƒí’ˆ ì •ë³´:\n" + "\n".join(product_summary))
        
        enriched_context = "\n\n".join(context_parts)
        state["enriched_context"] = enriched_context
        
        # React Agent í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = """**ë‹¹ì‹ ì€ ì „ë¬¸ ì‡¼í•‘ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.**

        **ì—­í• **: ì‚¬ìš©ìì—ê²Œ ìµœê³ ì˜ ì‡¼í•‘ ê²½í—˜ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤. ë‹¨ìˆœí•œ ìƒí’ˆ ë‚˜ì—´ì´ ì•„ë‹Œ, ê°œì¸í™”ëœ ë§ì¶¤ ì¶”ì²œì„ í†µí•´ ì‚¬ìš©ìê°€ ë§Œì¡±í•  ìˆ˜ ìˆëŠ” ì™„ë²½í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

        **ğŸ¯ ë‹µë³€ êµ¬ì„± ì›ì¹™**:

        **1. ê°œì¸í™”ëœ ì¸ì‚¬ ë° ìš”êµ¬ì‚¬í•­ í™•ì¸**
        - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì •í™•íˆ ì´í•´í–ˆìŒì„ ë³´ì—¬ì£¼ì„¸ìš”
        - ë¶„ì„ëœ ìš”êµ¬ì‚¬í•­ì„ ì¬í™•ì¸í•˜ë©° ê³µê°ëŒ€ í˜•ì„±

        **2. í•µì‹¬ ì¶”ì²œ ìƒí’ˆ**
        ê° ìƒí’ˆë§ˆë‹¤ ë‹¤ìŒ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì œê³µ:
        - **ìƒí’ˆëª…**: ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì œí’ˆëª…
        - **í•µì‹¬ íŠ¹ì§•**: ì™œ ì´ ìƒí’ˆì„ ì¶”ì²œí•˜ëŠ”ì§€ ëª…í™•í•œ ì´ìœ 
        - **ê°€ê²© ì •ë³´**: êµ¬ì²´ì ì¸ ìƒí’ˆ ê°€ê²©
        - **ì¥ì **: ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ê³¼ ì—°ê²°ëœ ì¥ì 
        - **ì£¼ì˜ì‚¬í•­**: ì†”ì§í•œ ë‹¨ì ì´ë‚˜ ê³ ë ¤ì‚¬í•­ (ì‹ ë¢°ì„± í–¥ìƒ)
        - **êµ¬ë§¤ì²˜**: êµ¬ì²´ì ì¸ ì˜¨ë¼ì¸ëª°ì´ë‚˜ êµ¬ë§¤ ë°©ë²•

        **3. ê°€ê²©ëŒ€ë³„ ì„¸ë¶„í™” ì¶”ì²œ**
        - **ê²½ì œì  ì„ íƒ**: ê°€ì„±ë¹„ ì¤‘ì‹¬ ì˜µì…˜
        - **ê· í˜• ì„ íƒ**: ê°€ê²©ê³¼ í’ˆì§ˆì˜ ê· í˜•
        - **í”„ë¦¬ë¯¸ì—„ ì„ íƒ**: ìµœê³  í’ˆì§ˆ/ì„±ëŠ¥ ì¤‘ì‹¬

        **4. ì‹¤ìš©ì  êµ¬ë§¤ ê°€ì´ë“œ**
        - **êµ¬ë§¤ ì‹œ ì²´í¬í¬ì¸íŠ¸**: ì‚¬ì´ì¦ˆ, ìƒ‰ìƒ, ë°°ì†¡, A/S ë“±
        - **ê³„ì ˆì„±/ì‹œê¸° ê³ ë ¤ì‚¬í•­**: ì–¸ì œ ì‚¬ëŠ” ê²ƒì´ ìœ ë¦¬í•œì§€
        - **ëŒ€ì•ˆ ìƒí’ˆ**: ì¬ê³  ë¶€ì¡±ì´ë‚˜ ì˜ˆì‚° ì´ˆê³¼ ì‹œ ëŒ€ì²´ì¬

        **5. ì „ë¬¸ê°€ íŒ & ê°œì¸í™” ì¡°ì–¸**
        - í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì „ë¬¸ì  ì¸ì‚¬ì´íŠ¸
        - ì‚¬ìš©ì ìƒí™©ì— ë§ëŠ” ë§ì¶¤ ì¡°ì–¸
        - í–¥í›„ êµ¬ë§¤ë¥¼ ìœ„í•œ íŠ¸ë Œë“œ ì •ë³´

        **ğŸ¨ ë‹µë³€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ**:
        - **ì¹œê·¼í•˜ê³  ì „ë¬¸ì **: ë”±ë”±í•˜ì§€ ì•Šìœ¼ë©´ì„œë„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í†¤
        - **êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì **: ëª¨í˜¸í•œ í‘œí˜„ë³´ë‹¤ëŠ” ëª…í™•í•œ ì •ë³´
        - **ê· í˜•ì¡íŒ ì‹œê°**: ì¥ì ë§Œì´ ì•„ë‹Œ ì†”ì§í•œ ë‹¨ì ë„ í¬í•¨
        - **í–‰ë™ ìœ ë„**: ì‚¬ìš©ìê°€ ë‹¤ìŒì— ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ëª…í™•íˆ ì œì‹œ

        **âš ï¸ ì£¼ì˜ì‚¬í•­**:
        - ìˆ˜ì§‘ëœ ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°, ì†”ì§í•˜ê²Œ í•œê³„ë¥¼ ì¸ì •í•˜ì„¸ìš”
        - ê³¼ì¥ëœ í‘œí˜„ë³´ë‹¤ëŠ” ê°ê´€ì  ì •ë³´ë¥¼ ìš°ì„ í•˜ì„¸ìš”
        - ê°€ê²©ì€ ë³€ë™ ê°€ëŠ¥í•¨ì„ ëª…ì‹œí•˜ì„¸ìš”
        - ê°œì¸ ì·¨í–¥ê³¼ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒì„ ì•ˆë‚´í•˜ì„¸ìš”

        **ğŸ“Š ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´**:
        {context}

        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìµœê³ ì˜ ì‡¼í•‘ ê²½í—˜ì„ ì„ ì‚¬í•˜ëŠ” ì™„ë²½í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        try:
            messages = [
                SystemMessage(content=system_prompt.format(context=enriched_context)),
                HumanMessage(content=state["user_query"])
            ]
            
            response = await self.llm.ainvoke(messages)
            state["final_answer"] = response.content
            state["processing_status"] = "ì²˜ë¦¬ ì™„ë£Œ"
            
            print(f"âœ… ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ ({len(response.content)}ì)")
            print(f"   - ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {response.content[:100]}...")
            
            # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            state["messages"].extend([
                HumanMessage(content=state["user_query"]),
                response
            ])
            
        except Exception as e:
            print(f"âŒ React Agent ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            state["error_info"] = f"React Agent ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
            state["processing_status"] = "ì²˜ë¦¬ ì‹¤íŒ¨"
            state["final_answer"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
        print("\n=== ğŸ‰ Enhanced Shopping Agent ì²˜ë¦¬ ì™„ë£Œ ===\n")
        return state


# ë„êµ¬ í•¨ìˆ˜ë“¤
@tool
def get_current_time() -> str:
    """í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


async def build_enhanced_agent(config_name: str = "credit_saving") -> CompiledStateGraph:
    """Enhanced Shopping Agent ë¹Œë“œ"""
    config = get_config(config_name)
    agent = EnhancedShoppingAgent(config)
    return agent.create_workflow()


async def run_agent_test():
    """ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    agent = await build_enhanced_agent()
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_query = "ê²¨ìš¸ìš© ë”°ëœ»í•œ íŒ¨ë”© ì í¼ ì¶”ì²œí•´ì¤˜. 10ë§Œì› ì´í•˜ë¡œ ê²€ì€ìƒ‰ì´ë©´ ì¢‹ê² ì–´."
    
    initial_state = {
        "user_query": test_query,
        "messages": [],
        "processing_status": "ì‹œì‘"
    }
    
    print("=== Enhanced Shopping Agent í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    print(f"ì§ˆë¬¸: {test_query}")
    print()
    
    # ì—ì´ì „íŠ¸ ì‹¤í–‰
    async for chunk in agent.astream(
        initial_state,
        stream_mode="values"
    ):
        messages = chunk["messages"]

        for msg in messages:
            msg.pretty_print()

    # result = await agent.ainvoke(initial_state)
    
    # # ê²°ê³¼ ì¶œë ¥
    # print(f"ì²˜ë¦¬ ìƒíƒœ: {result.get('processing_status')}")
    # print(f"ìµœì¢… ë‹µë³€: {result.get('final_answer')}")
    
    # if result.get('error_info'):
    #     print(f"ì˜¤ë¥˜ ì •ë³´: {result['error_info']}")
    
    # # ìƒì„¸ ì •ë³´ ì¶œë ¥
    # print("\n=== ì²˜ë¦¬ ê³¼ì • ìƒì„¸ ===")
    # if result.get('analyzed_query'):
    #     print(f"ì§ˆë¬¸ ë¶„ì„: {json.dumps(result['analyzed_query'], ensure_ascii=False, indent=2)}")
    
    # if result.get('search_results'):
    #     print(f"ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(result['search_results'])}")
    
    # if result.get('product_data'):
    #     print(f"ìˆ˜ì§‘ëœ ìƒí’ˆ ìˆ˜: {len(result['product_data'])}")


if __name__ == "__main__":
    asyncio.run(run_agent_test())